{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films (Train Data, Naive Bayes)\t Accuracy (All)=0.82 (7849/9598)\n",
      "\n",
      "Films (Train Data, Naive Bayes)\t Precision (Pos)=0.93 (3288/3520)\n",
      "Films (Train Data, Naive Bayes)\t Recall (Pos)=0.68 (3288/4805)\n",
      "Films (Train Data, Naive Bayes)\t F-measure (Pos)=0.79\n",
      "Films (Train Data, Naive Bayes)\t Precision (Neg)=0.75 (4561/6078)\n",
      "Films (Train Data, Naive Bayes)\t Recall (Neg)=0.95 (4561/4793)\n",
      "Films (Train Data, Naive Bayes)\t F-measure (Neg)=0.84\n",
      "\n",
      "Films  (Test Data, Naive Bayes)\t Accuracy (All)=0.77 (825/1065)\n",
      "\n",
      "Films  (Test Data, Naive Bayes)\t Precision (Pos)=0.78 (394/502)\n",
      "Films  (Test Data, Naive Bayes)\t Recall (Pos)=0.75 (394/526)\n",
      "Films  (Test Data, Naive Bayes)\t F-measure (Pos)=0.77\n",
      "Films  (Test Data, Naive Bayes)\t Precision (Neg)=0.77 (431/563)\n",
      "Films  (Test Data, Naive Bayes)\t Recall (Neg)=0.80 (431/539)\n",
      "Films  (Test Data, Naive Bayes)\t F-measure (Neg)=0.78\n",
      "\n",
      "Nokia   (All Data,  Naive Bayes)\t Accuracy (All)=0.62 (164/266)\n",
      "\n",
      "Nokia   (All Data,  Naive Bayes)\t Precision (Pos)=0.81 (109/134)\n",
      "Nokia   (All Data,  Naive Bayes)\t Recall (Pos)=0.59 (109/186)\n",
      "Nokia   (All Data,  Naive Bayes)\t F-measure (Pos)=0.68\n",
      "Nokia   (All Data,  Naive Bayes)\t Precision (Neg)=0.42 (55/132)\n",
      "Nokia   (All Data,  Naive Bayes)\t Recall (Neg)=0.69 (55/80)\n",
      "Nokia   (All Data,  Naive Bayes)\t F-measure (Neg)=0.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import re, random, math, collections, itertools\n",
    "\n",
    "PRINT_ERRORS=0\n",
    "\n",
    "#------------- Function Definitions ---------------------\n",
    "\n",
    "\n",
    "def readFiles(sentimentDictionary,sentencesTrain,sentencesTest,sentencesNokia):\n",
    "\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    posSentences = open('rt-polarity.pos', 'r', encoding=\"ISO-8859-1\")\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "\n",
    "    negSentences = open('rt-polarity.neg', 'r', encoding=\"ISO-8859-1\")\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "\n",
    "    posSentencesNokia = open('nokia-pos.txt', 'r')\n",
    "    posSentencesNokia = re.split(r'\\n', posSentencesNokia.read())\n",
    "\n",
    "    negSentencesNokia = open('nokia-neg.txt', 'r', encoding=\"ISO-8859-1\")\n",
    "    negSentencesNokia = re.split(r'\\n', negSentencesNokia.read())\n",
    "\n",
    "    posDictionary = open('positive-words.txt', 'r', encoding=\"ISO-8859-1\")\n",
    "    posWordList = posDictionary.readlines()\n",
    "    posWordList = [line.strip() for line in posWordList if not line.startswith(\";\") and not line == '\\n']\n",
    "    #posWordList = re.findall(r\"[a-z\\-]+\", posDictionary.read())\n",
    "\n",
    "    negDictionary = open('negative-words.txt', 'r', encoding=\"ISO-8859-1\")\n",
    "    negWordList = negDictionary.readlines()\n",
    "    negWordList = [line.strip() for line in negWordList if not line.startswith(\";\") and not line == '\\n']\n",
    "    #negWordList = re.findall(r\"[a-z\\-]+\", negDictionary.read())\n",
    "\n",
    "    for i in posWordList:\n",
    "        sentimentDictionary[i] = 1\n",
    "    for i in negWordList:\n",
    "        sentimentDictionary[i] = -1\n",
    "\n",
    "    #create Training and Test Datsets:\n",
    "    #We want to test on sentences we haven't trained on, to see how well the model generalses to previously unseen sentences\n",
    "\n",
    "  #create 90-10 split of training and test data from movie reviews, with sentiment labels    \n",
    "    for i in posSentences:\n",
    "        if random.randint(1,10)<2:\n",
    "            sentencesTest[i]=\"positive\"\n",
    "        else:\n",
    "            sentencesTrain[i]=\"positive\"\n",
    "\n",
    "    for i in negSentences:\n",
    "        if random.randint(1,10)<2:\n",
    "            sentencesTest[i]=\"negative\"\n",
    "        else:\n",
    "            sentencesTrain[i]=\"negative\"\n",
    "\n",
    "    #create Nokia Datset:\n",
    "    for i in posSentencesNokia:\n",
    "            sentencesNokia[i]=\"positive\"\n",
    "    for i in negSentencesNokia:\n",
    "            sentencesNokia[i]=\"negative\"\n",
    "\n",
    "#----------------------------End of data initialisation ----------------#\n",
    "\n",
    "#calculates p(W|Positive), p(W|Negative) and p(W) for all words in training data\n",
    "def trainBayes(sentencesTrain, pWordPos, pWordNeg, pWord):\n",
    "    posFeatures = [] # [] initialises a list [array]\n",
    "    negFeatures = [] \n",
    "    freqPositive = {} # {} initialises a dictionary [hash function]\n",
    "    freqNegative = {}\n",
    "    dictionary = {}\n",
    "    posWordsTot = 0\n",
    "    negWordsTot = 0\n",
    "    allWordsTot = 0\n",
    "\n",
    "    #iterate through each sentence/sentiment pair in the training data\n",
    "    for sentence, sentiment in sentencesTrain.items():\n",
    "        wordList = re.findall(r\"[\\w']+\", sentence)\n",
    "\n",
    "        #TO DO:\n",
    "        #Populate bigramList (initialised below) by concatenating adjacent words in the sentence.\n",
    "        #You might want to seperate the words by _ for readability, so bigrams such as:\n",
    "        #You_might, might_want, want_to, to_seperate.... \n",
    "\n",
    "        bigramList=wordList.copy() #initialise bigramList\n",
    "        for x in range(len(wordList)-1):\n",
    "           bigramList.append(wordList[x]+\"_\" + wordList[x+1])\n",
    "        \n",
    "        trigramList=bigramList.copy() #initialise bigramList\n",
    "        for x in range(len(wordList)-2):\n",
    "            trigramList.append(wordList[x]+\"\" + wordList[x+1]+\"\" + wordList[x+2])\n",
    "\n",
    "\n",
    " \n",
    "        #-------------Finish populating bigramList ------------------#\n",
    "        \n",
    "        #TO DO: when you have populated bigramList, uncomment out the line below and , and comment out the unigram line to make use of bigramList rather than wordList\n",
    "        \n",
    "        for word in  trigramList: #calculate over bigrams\n",
    "        # for word in wordList: #calculate over unigrams\n",
    "            allWordsTot += 1 # keeps count of total words in dataset\n",
    "            if not (word in dictionary):\n",
    "                dictionary[word] = 1\n",
    "            if sentiment==\"positive\" :\n",
    "                posWordsTot += 1 # keeps count of total words in positive class\n",
    "\n",
    "                #keep count of each word in positive context\n",
    "                if not (word in freqPositive):\n",
    "                    freqPositive[word] = 1\n",
    "                else:\n",
    "                    freqPositive[word] += 1    \n",
    "            else:\n",
    "                negWordsTot+=1# keeps count of total words in negative class\n",
    "                \n",
    "                #keep count of each word in positive context\n",
    "                if not (word in freqNegative):\n",
    "                    freqNegative[word] = 1\n",
    "                else:\n",
    "                    freqNegative[word] += 1\n",
    "\n",
    "    for word in dictionary:\n",
    "        #do some smoothing so that minimum count of a word is 1\n",
    "        if not (word in freqNegative):\n",
    "            freqNegative[word] = 1\n",
    "        if not (word in freqPositive):\n",
    "            freqPositive[word] = 1\n",
    "\n",
    "        # Calculate p(word|positive)\n",
    "        pWordPos[word] = freqPositive[word] / float(posWordsTot)\n",
    "\n",
    "        # Calculate p(word|negative) \n",
    "        pWordNeg[word] = freqNegative[word] / float(negWordsTot)\n",
    "\n",
    "        # Calculate p(word)\n",
    "        pWord[word] = (freqPositive[word] + freqNegative[word]) / float(allWordsTot) \n",
    "\n",
    "#---------------------------End Training ----------------------------------\n",
    "\n",
    "#implement naive bayes algorithm\n",
    "#INPUTS:\n",
    "#  sentencesTest is a dictonary with sentences associated with sentiment \n",
    "#  dataName is a string (used only for printing output)\n",
    "#  pWordPos is dictionary storing p(word|positive) for each word\n",
    "#     i.e., pWordPos[\"apple\"] will return a real value for p(\"apple\"|positive)\n",
    "#  pWordNeg is dictionary storing p(word|negative) for each word\n",
    "#  pWord is dictionary storing p(word)\n",
    "#  pPos is a real number containing the fraction of positive reviews in the dataset\n",
    "def testBayes(sentencesTest, dataName, pWordPos, pWordNeg, pWord,pPos):\n",
    "    pNeg=1-pPos\n",
    "\n",
    "    #These variables will store results (you do not need them)\n",
    "    total=0\n",
    "    correct=0\n",
    "    totalpos=0\n",
    "    totalpospred=0\n",
    "    totalneg=0\n",
    "    totalnegpred=0\n",
    "    correctpos=0\n",
    "    correctneg=0\n",
    "\n",
    "    #for each sentence, sentiment pair in the dataset\n",
    "    for sentence, sentiment in sentencesTest.items():\n",
    "        wordList = re.findall(r\"[\\w']+\", sentence)#collect all words\n",
    "\n",
    "        \n",
    "        #TO DO: Exactly what you did in the training function:\n",
    "        #Populate bigramList by concatenating adjacent words in wordList.\n",
    "\n",
    "        bigramList=wordList.copy() #initialise bigramList\n",
    "        for x in range(len(wordList)-1):\n",
    "            bigramList.append(wordList[x]+\"_\" + wordList[x+1])\n",
    "        trigramList=bigramList.copy() #initialise bigramList\n",
    "        for x in range(len(wordList)-2):\n",
    "            trigramList.append(wordList[x]+\"\" + wordList[x+1]+\"\" + wordList[x+2])\n",
    "        \n",
    "        \n",
    "\n",
    "#------------------finished populating bigramList--------------\n",
    "        pPosW=pPos\n",
    "        pNegW=pNeg\n",
    "\n",
    "        for word in  trigramList: #calculate over bigrams\n",
    "#        for word in wordList: #calculate over unigrams\n",
    "            if word in pWord:\n",
    "                if pWord[word]>0.00000001:\n",
    "                    pPosW *=pWordPos[word]\n",
    "                    pNegW *=pWordNeg[word]\n",
    "\n",
    "        prob=0;            \n",
    "        if pPosW+pNegW >0:\n",
    "            prob=pPosW/float(pPosW+pNegW)\n",
    "\n",
    "\n",
    "        total+=1\n",
    "        if sentiment==\"positive\":\n",
    "            totalpos+=1\n",
    "            if prob>0.5:\n",
    "                correct+=1\n",
    "                correctpos+=1\n",
    "                totalpospred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalnegpred+=1\n",
    "                if PRINT_ERRORS:\n",
    "                    print (\"ERROR (pos classed as neg %0.2f):\" %prob + sentence)\n",
    "        else:\n",
    "            totalneg+=1\n",
    "            if prob<=0.5:\n",
    "                correct+=1\n",
    "                correctneg+=1\n",
    "                totalnegpred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalpospred+=1\n",
    "                if PRINT_ERRORS:\n",
    "                    print (\"ERROR (neg classed as pos %0.2f):\" %prob + sentence)\n",
    " \n",
    "    acc=correct/float(total)\n",
    "    print (dataName + \" Accuracy (All)=%0.2f\" % acc + \" (%d\" % correct + \"/%d\" % total + \")\\n\")\n",
    "\n",
    "    precision_pos=correctpos/float(totalpospred)\n",
    "    recall_pos=correctpos/float(totalpos)\n",
    "    precision_neg=correctneg/float(totalnegpred)\n",
    "    recall_neg=correctneg/float(totalneg)\n",
    "    f_pos=2*precision_pos*recall_pos/(precision_pos+recall_pos);\n",
    "    f_neg=2*precision_neg*recall_neg/(precision_neg+recall_neg);\n",
    "\n",
    "    print (dataName + \" Precision (Pos)=%0.2f\" % precision_pos + \" (%d\" % correctpos + \"/%d\" % totalpospred + \")\")\n",
    "    print (dataName + \" Recall (Pos)=%0.2f\" % recall_pos + \" (%d\" % correctpos + \"/%d\" % totalpos + \")\")\n",
    "    print (dataName + \" F-measure (Pos)=%0.2f\" % f_pos)\n",
    "\n",
    "    print (dataName + \" Precision (Neg)=%0.2f\" % precision_neg + \" (%d\" % correctneg + \"/%d\" % totalnegpred + \")\")\n",
    "    print (dataName + \" Recall (Neg)=%0.2f\" % recall_neg + \" (%d\" % correctneg + \"/%d\" % totalneg + \")\")\n",
    "    print (dataName + \" F-measure (Neg)=%0.2f\" % f_neg + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# This is a simple classifier that uses a sentiment dictionary to classify \n",
    "# a sentence. For each word in the sentence, if the word is in the positive \n",
    "# dictionary, it adds 1, if it is in the negative dictionary, it subtracts 1. \n",
    "# If the final score is above a threshold, it classifies as \"Positive\", \n",
    "# otherwise as \"Negative\"\n",
    "def testDictionary(sentencesTest, dataName, sentimentDictionary, threshold):\n",
    "    total=0\n",
    "    correct=0\n",
    "    totalpos=0\n",
    "    totalneg=0\n",
    "    totalpospred=0\n",
    "    totalnegpred=0\n",
    "    correctpos=0\n",
    "    correctneg=0\n",
    "    for sentence, sentiment in sentencesTest.items():\n",
    "        Words = re.findall(r\"[\\w']+\", sentence)\n",
    "        score=0\n",
    "        for word in Words:\n",
    "            if word in sentimentDictionary:\n",
    "               score+=sentimentDictionary[word]\n",
    " \n",
    "        total+=1\n",
    "        if sentiment==\"positive\":\n",
    "            totalpos+=1\n",
    "            if score>=threshold:\n",
    "                correct+=1\n",
    "                correctpos+=1\n",
    "                totalpospred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalnegpred+=1\n",
    "        else:\n",
    "            totalneg+=1\n",
    "            if score<threshold:\n",
    "                correct+=1\n",
    "                correctneg+=1\n",
    "                totalnegpred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalpospred+=1\n",
    " \n",
    "    acc=correct/float(total)\n",
    "    print (dataName + \" Accuracy (All)=%0.2f\" % acc + \" (%d\" % correct + \"/%d\" % total + \")\\n\")\n",
    "    precision_pos=correctpos/float(totalpospred)\n",
    "    recall_pos=correctpos/float(totalpos)\n",
    "    precision_neg=correctneg/float(totalnegpred)\n",
    "    recall_neg=correctneg/float(totalneg)\n",
    "    f_pos=2*precision_pos*recall_pos/(precision_pos+recall_pos);\n",
    "    f_neg=2*precision_neg*recall_neg/(precision_neg+recall_neg);\n",
    "\n",
    "\n",
    "    print (dataName + \" Precision (Pos)=%0.2f\" % precision_pos + \" (%d\" % correctpos + \"/%d\" % totalpospred + \")\")\n",
    "    print (dataName + \" Recall (Pos)=%0.2f\" % recall_pos + \" (%d\" % correctpos + \"/%d\" % totalpos + \")\")\n",
    "    print (dataName + \" F-measure (Pos)=%0.2f\" % f_pos)\n",
    "\n",
    "    print (dataName + \" Precision (Neg)=%0.2f\" % precision_neg + \" (%d\" % correctneg + \"/%d\" % totalnegpred + \")\")\n",
    "    print (dataName + \" Recall (Neg)=%0.2f\" % recall_neg + \" (%d\" % correctneg + \"/%d\" % totalneg + \")\")\n",
    "    print (dataName + \" F-measure (Neg)=%0.2f\" % f_neg + \"\\n\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#Print out n most useful predictors\n",
    "def mostUseful(pWordPos, pWordNeg, pWord, n):\n",
    "    predictPower={}\n",
    "    for word in pWord:\n",
    "        if pWordNeg[word]<0.0000001:\n",
    "            predictPower[word]=1000000000\n",
    "        else:\n",
    "            predictPower[word]=pWordPos[word] / (pWordPos[word] + pWordNeg[word])\n",
    "            \n",
    "\n",
    "    sortedPower = sorted(predictPower, key=predictPower.get)\n",
    "    head, tail = sortedPower[:n], sortedPower[len(predictPower)-n:]\n",
    "    print (\"NEGATIVE:\")\n",
    "    print (head)\n",
    "    print (\"\\nPOSITIVE:\")\n",
    "    print (tail)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------- Main Script --------------------------\n",
    "\n",
    "\n",
    "sentimentDictionary={} # {} initialises a dictionary [hash function]\n",
    "sentencesTrain={}\n",
    "sentencesTest={}\n",
    "sentencesNokia={}\n",
    "\n",
    "#initialise datasets and dictionaries\n",
    "readFiles(sentimentDictionary,sentencesTrain,sentencesTest,sentencesNokia)\n",
    "\n",
    "pWordPos={} # p(W|Positive)\n",
    "pWordNeg={} # p(W|Negative)\n",
    "pWord={}    # p(W) \n",
    "\n",
    "#build conditional probabilities using training data\n",
    "trainBayes(sentencesTrain, pWordPos, pWordNeg, pWord)\n",
    "\n",
    "#run naive bayes classifier on datasets\n",
    "#print (\"Naive Bayes\")\n",
    "testBayes(sentencesTrain,  \"Films (Train Data, Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.5)\n",
    "testBayes(sentencesTest,  \"Films  (Test Data, Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.5)\n",
    "testBayes(sentencesNokia, \"Nokia   (All Data,  Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.7)\n",
    "\n",
    "\n",
    "\n",
    "#run sentiment dictionary based classifier on datasets\n",
    "#testDictionary(sentencesTrain,  \"Films (Train Data, Rule-Based)\\t\", sentimentDictionary, -4)\n",
    "#testDictionary(sentencesTest,  \"Films  (Test Data, Rule-Based)\\t\",  sentimentDictionary, 0)\n",
    "#testDictionary(sentencesNokia, \"Nokia   (All Data, Rule-Based)\\t\",  sentimentDictionary, 0)\n",
    "\n",
    "\n",
    "# print most useful words\n",
    "#mostUseful(pWordPos, pWordNeg, pWord, 50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDictionary_new_rules(sentencesTest, dataName, sentimentDictionary, threshold, score_count=2):\n",
    "    total=0\n",
    "    correct=0\n",
    "    totalpos=0\n",
    "    totalneg=0\n",
    "    totalpospred=0\n",
    "    totalnegpred=0\n",
    "    correctpos=0\n",
    "    correctneg=0\n",
    "    count=0\n",
    "    lst = []\n",
    "    for sentence, sentiment in sentencesTest.items():\n",
    "        Words = re.findall(r\"[\\w']+\", sentence)\n",
    "        score=0\n",
    "        left_split = []\n",
    "        not_list = []\n",
    "        for word in Words:\n",
    "            if 'but' in Words:\n",
    "                for i in range(len(Words) - 1):\n",
    "                    left_split.append(Words[i])\n",
    "                    if Words[i + 1] == 'but':\n",
    "                        break\n",
    "            \n",
    "            elif 'not bad' in Words:\n",
    "                for i in range(len(Words) - 1):\n",
    "                    left_split.append(Words[i])\n",
    "                    if Words[i + 1] == 'not bad':\n",
    "                        break\n",
    "                        \n",
    "            elif 'only' in Words:\n",
    "                for i in range(len(Words) - 1):\n",
    "                    left_split.append(Words[i])\n",
    "                    if Words[i + 1] == 'only':\n",
    "                        break\n",
    "\n",
    "            \n",
    "                #print(left_split)\n",
    "        \n",
    "                if len(left_split) > 0:\n",
    "                    for word in left_split:\n",
    "                        if word in sentimentDictionary:\n",
    "                            if sentimentDictionary[word] == -1:\n",
    "                                score += score_count\n",
    "                            elif sentimentDictionary[word] == 1:\n",
    "                                score -= score_count\n",
    "                            else:\n",
    "                                score +=0\n",
    "                            count+=1\n",
    "                            #print(score)     \n",
    "            elif word in sentimentDictionary:\n",
    "                score+=sentimentDictionary[word]\n",
    "                \n",
    "        #print(sentence, sentiment)\n",
    "        total+=1\n",
    "        if sentiment==\"positive\":\n",
    "            totalpos+=1\n",
    "            if score>=threshold:\n",
    "                correct+=1\n",
    "                correctpos+=1\n",
    "                totalpospred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalnegpred+=1\n",
    "        else:\n",
    "            totalneg+=1\n",
    "            if score<threshold:\n",
    "                correct+=1\n",
    "                correctneg+=1\n",
    "                totalnegpred+=1\n",
    "            else:\n",
    "                correct+=0\n",
    "                totalpospred+=1\n",
    "\n",
    "    acc=correct/float(total)\n",
    "    print (dataName + \" Accuracy (All)=%0.2f\" % acc + \" (%d\" % correct + \"/%d\" % total + \")\\n\")\n",
    "    precision_pos=correctpos/float(totalpospred)\n",
    "    recall_pos=correctpos/float(totalpos)\n",
    "    precision_neg=correctneg/float(totalnegpred) if (totalnegpred) else 0\n",
    "    recall_neg=correctneg/float(totalneg) #if (totalneg) else 0\n",
    "    f_pos=2*precision_pos*recall_pos/(precision_pos+recall_pos);\n",
    "    f_neg=2*precision_neg*recall_neg/(precision_neg+recall_neg) if (precision_neg+recall_neg) else 0;\n",
    "\n",
    "\n",
    "    print (dataName + \" Precision (Pos)=%0.2f\" % precision_pos + \" (%d\" % correctpos + \"/%d\" % totalpospred + \")\")\n",
    "    print (dataName + \" Recall (Pos)=%0.2f\" % recall_pos + \" (%d\" % correctpos + \"/%d\" % totalpos + \")\")\n",
    "    print (dataName + \" F-measure (Pos)=%0.2f\" % f_pos)\n",
    "\n",
    "    print (dataName + \" Precision (Neg)=%0.2f\" % precision_neg + \" (%d\" % correctneg + \"/%d\" % totalnegpred + \")\")\n",
    "    print (dataName + \" Recall (Neg)=%0.2f\" % recall_neg + \" (%d\" % correctneg + \"/%d\" % totalneg + \")\")\n",
    "    print (dataName + \" F-measure (Neg)=%0.2f\" % f_neg + \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films (Train Data, Rule-Based)\t Accuracy (All)=0.64 (6122/9598)\n",
      "\n",
      "Films (Train Data, Rule-Based)\t Precision (Pos)=0.70 (2316/3303)\n",
      "Films (Train Data, Rule-Based)\t Recall (Pos)=0.48 (2316/4805)\n",
      "Films (Train Data, Rule-Based)\t F-measure (Pos)=0.57\n",
      "Films (Train Data, Rule-Based)\t Precision (Neg)=0.60 (3806/6295)\n",
      "Films (Train Data, Rule-Based)\t Recall (Neg)=0.79 (3806/4793)\n",
      "Films (Train Data, Rule-Based)\t F-measure (Neg)=0.69\n",
      "\n",
      "Films  (Test Data, Rule-Based)\t Accuracy (All)=0.65 (694/1065)\n",
      "\n",
      "Films  (Test Data, Rule-Based)\t Precision (Pos)=0.72 (254/353)\n",
      "Films  (Test Data, Rule-Based)\t Recall (Pos)=0.48 (254/526)\n",
      "Films  (Test Data, Rule-Based)\t F-measure (Pos)=0.58\n",
      "Films  (Test Data, Rule-Based)\t Precision (Neg)=0.62 (440/712)\n",
      "Films  (Test Data, Rule-Based)\t Recall (Neg)=0.82 (440/539)\n",
      "Films  (Test Data, Rule-Based)\t F-measure (Neg)=0.70\n",
      "\n",
      "Nokia   (All Data, Rule-Based)\t Accuracy (All)=0.80 (212/266)\n",
      "\n",
      "Nokia   (All Data, Rule-Based)\t Precision (Pos)=0.92 (145/158)\n",
      "Nokia   (All Data, Rule-Based)\t Recall (Pos)=0.78 (145/186)\n",
      "Nokia   (All Data, Rule-Based)\t F-measure (Pos)=0.84\n",
      "Nokia   (All Data, Rule-Based)\t Precision (Neg)=0.62 (67/108)\n",
      "Nokia   (All Data, Rule-Based)\t Recall (Neg)=0.84 (67/80)\n",
      "Nokia   (All Data, Rule-Based)\t F-measure (Neg)=0.71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDictionary_new_rules(sentencesTrain,  \"Films (Train Data, Rule-Based)\\t\", sentimentDictionary, 1)\n",
    "testDictionary_new_rules(sentencesTest,  \"Films  (Test Data, Rule-Based)\\t\",  sentimentDictionary, 1)\n",
    "testDictionary_new_rules(sentencesNokia, \"Nokia   (All Data, Rule-Based)\\t\",  sentimentDictionary, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding trigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDictionary_tri(sentencesTest, dataName, sentimentDictionary, threshold):\n",
    "    total=0\n",
    "    correct=0\n",
    "    totalpos=0\n",
    "    totalneg=0\n",
    "    totalpospred=0\n",
    "    totalnegpred=0\n",
    "    correctpos=0\n",
    "    correctneg=0\n",
    "    for sentence, sentiment in sentencesTest.items():\n",
    "        wordList = re.findall(r\"[\\w']+\", sentence)\n",
    "        trigramList = wordList.copy()  # initialise trigramList\n",
    "        for x in range(len(wordList) - 2):\n",
    "              trigramList.append(wordList[x] + \"_\" + wordList[x + 1] + \"_\" + wordList[x + 2])\n",
    "        score=0\n",
    "        for trigram in trigramList:\n",
    "            Words = trigram.split(\"_\")\n",
    "            for word in Words:\n",
    "                if word in sentimentDictionary:\n",
    "                   score+=sentimentDictionary[word]\n",
    "            total+=1\n",
    "            if sentiment==\"positive\":\n",
    "                totalpos+=1\n",
    "                if score>=threshold:\n",
    "                    correct+=1\n",
    "                    correctpos+=1\n",
    "                    totalpospred+=1\n",
    "                else:\n",
    "                    correct+=0\n",
    "                    totalnegpred+=1\n",
    "            else:\n",
    "                totalneg+=1\n",
    "                if score<threshold:\n",
    "                    correct+=1\n",
    "                    correctneg+=1\n",
    "                    totalnegpred+=1\n",
    "                else:\n",
    "                    correct+=0\n",
    "                    totalpospred+=1\n",
    " \n",
    "    acc=correct/float(total)\n",
    "    print (dataName + \" Accuracy (All)=%0.2f\" % acc + \" (%d\" % correct + \"/%d\" % total + \")\\n\")\n",
    "    precision_pos=correctpos/float(totalpospred)\n",
    "    recall_pos=correctpos/float(totalpos)\n",
    "    precision_neg=correctneg/float(totalnegpred)\n",
    "    recall_neg=correctneg/float(totalneg)\n",
    "    f_pos=2*precision_pos*recall_pos/(precision_pos+recall_pos);\n",
    "    f_neg=2*precision_neg*recall_neg/(precision_neg+recall_neg);\n",
    "\n",
    "\n",
    "    print (dataName + \" Precision (Pos)=%0.2f\" % precision_pos + \" (%d\" % correctpos + \"/%d\" % totalpospred + \")\")\n",
    "    print (dataName + \" Recall (Pos)=%0.2f\" % recall_pos + \" (%d\" % correctpos + \"/%d\" % totalpos + \")\")\n",
    "    print (dataName + \" F-measure (Pos)=%0.2f\" % f_pos)\n",
    "\n",
    "    print (dataName + \" Precision (Neg)=%0.2f\" % precision_neg + \" (%d\" % correctneg + \"/%d\" % totalnegpred + \")\")\n",
    "    print (dataName + \" Recall (Neg)=%0.2f\" % recall_neg + \" (%d\" % correctneg + \"/%d\" % totalneg + \")\")\n",
    "    print (dataName + \" F-measure (Neg)=%0.2f\" % f_neg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films (Train Data, Rule-Based)\t Accuracy (All)=0.61 (210848/344308)\n",
      "\n",
      "Films (Train Data, Rule-Based)\t Precision (Pos)=0.64 (88692/138049)\n",
      "Films (Train Data, Rule-Based)\t Recall (Pos)=0.51 (88692/172795)\n",
      "Films (Train Data, Rule-Based)\t F-measure (Pos)=0.57\n",
      "Films (Train Data, Rule-Based)\t Precision (Neg)=0.59 (122156/206259)\n",
      "Films (Train Data, Rule-Based)\t Recall (Neg)=0.71 (122156/171513)\n",
      "Films (Train Data, Rule-Based)\t F-measure (Neg)=0.65\n",
      "\n",
      "Films  (Test Data, Rule-Based)\t Accuracy (All)=0.61 (22657/37138)\n",
      "\n",
      "Films  (Test Data, Rule-Based)\t Precision (Pos)=0.64 (8916/14025)\n",
      "Films  (Test Data, Rule-Based)\t Recall (Pos)=0.49 (8916/18288)\n",
      "Films  (Test Data, Rule-Based)\t F-measure (Pos)=0.55\n",
      "Films  (Test Data, Rule-Based)\t Precision (Neg)=0.59 (13741/23113)\n",
      "Films  (Test Data, Rule-Based)\t Recall (Neg)=0.73 (13741/18850)\n",
      "Films  (Test Data, Rule-Based)\t F-measure (Neg)=0.65\n",
      "\n",
      "Nokia   (All Data, Rule-Based)\t Accuracy (All)=0.70 (5457/7827)\n",
      "\n",
      "Nokia   (All Data, Rule-Based)\t Precision (Pos)=0.85 (3323/3919)\n",
      "Nokia   (All Data, Rule-Based)\t Recall (Pos)=0.65 (3323/5097)\n",
      "Nokia   (All Data, Rule-Based)\t F-measure (Pos)=0.74\n",
      "Nokia   (All Data, Rule-Based)\t Precision (Neg)=0.55 (2134/3908)\n",
      "Nokia   (All Data, Rule-Based)\t Recall (Neg)=0.78 (2134/2730)\n",
      "Nokia   (All Data, Rule-Based)\t F-measure (Neg)=0.64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDictionary_tri(sentencesTrain,  \"Films (Train Data, Rule-Based)\\t\", sentimentDictionary, 1)\n",
    "testDictionary_tri(sentencesTest,  \"Films  (Test Data, Rule-Based)\\t\",  sentimentDictionary, 1)\n",
    "testDictionary_tri(sentencesNokia, \"Nokia   (All Data, Rule-Based)\\t\",  sentimentDictionary, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE:\n",
      "['unfunny', 'supposed', 'generic', 'mediocre', 'badly', 'routine', 'supposed_to', 'the_problem', 'waste', 'mindless', 'poorly', 'has_no', '90_minutes', 'ends_up', 'pointless', 'boring', 'stale', 'disguise', 'too_bad', 'only_thing']\n",
      "\n",
      "POSITIVE:\n",
      "['an_engaging', 'gem', 'portraitofa', 'wonderful', 'chilling', 'love_and', 'refreshingly', 'performances_from', 'in_years', 'riveting', 'refreshing', 'mesmerizing', 'realistic', 'a_smart', 'unique', 'what_makes', 'lookatthe', 'inventive', 'engrossing', 'ofthebest']\n"
     ]
    }
   ],
   "source": [
    "mostUseful(pWordPos, pWordNeg, pWord, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking how many words used from sentiment dictonary to dictonary based approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_tail(pWordPos, pWordNeg, pWord, n):\n",
    "    predictPower={}\n",
    "    for word in pWord:\n",
    "        if pWordNeg[word]<0.0000001:\n",
    "            predictPower[word]=1000000000\n",
    "        else:\n",
    "            predictPower[word]=pWordPos[word] / (pWordPos[word] + pWordNeg[word])\n",
    "\n",
    "\n",
    "    sortedPower = sorted(predictPower, key=predictPower.get)\n",
    "    head, tail = sortedPower[:n], sortedPower[len(predictPower)-n:]\n",
    "    \n",
    "    return head, tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Words in SentimentDictionary are\n",
      "\n",
      "mediocre\n",
      "badly\n",
      "waste\n",
      "mindless\n",
      "poorly\n",
      "pointless\n",
      "boring\n",
      "stale\n",
      "bore\n",
      "stupid\n",
      "tiresome\n",
      "dull\n",
      "clumsy\n",
      "inept\n",
      "offensive\n",
      "lousy\n",
      "junk\n",
      "horrible\n",
      "annoying\n",
      "pathetic\n",
      "sadly\n",
      "incoherent\n",
      "banal\n",
      "missed\n",
      "crap\n",
      "stunt\n",
      " \n",
      "Positive Words in SentimentDictionary are\n",
      "\n",
      "heartbreaking\n",
      "hopeful\n",
      "smarter\n",
      "sadness\n",
      "powerful\n",
      "warm\n",
      "evocative\n",
      "captivating\n",
      "unexpected\n",
      "timely\n",
      "polished\n",
      "respect\n",
      "heartwarming\n",
      "subversive\n",
      "jealousy\n",
      "dazzling\n",
      "wonderfully\n",
      "beauty\n",
      "playful\n",
      "lively\n",
      "extraordinary\n",
      "tender\n",
      "gem\n",
      "wonderful\n",
      "refreshing\n",
      "mesmerizing\n",
      "realistic\n",
      "inventive\n",
      "engrossing\n",
      "total positive words in sentimentDictionary are: 29  total  words 100\n",
      "total negative words in sentimentDictionary are: 26  total  words 100\n"
     ]
    }
   ],
   "source": [
    "pcnt=0\n",
    "ncnt=0\n",
    "head, tail = head_tail(pWordPos, pWordNeg, pWord, 100)\n",
    "print('Negative Words in SentimentDictionary are')\n",
    "print('')\n",
    "for i in range(len(head)):\n",
    "    if head[i] in sentimentDictionary:\n",
    "        ncnt+=1\n",
    "        print(head[i])\n",
    "    else:\n",
    "        pass\n",
    "print(' ')\n",
    "print('Positive Words in SentimentDictionary are')\n",
    "print('')\n",
    "for i in range(len(tail)):\n",
    "    if tail[i] in sentimentDictionary:\n",
    "        pcnt+=1\n",
    "        print(tail[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('total positive words in sentimentDictionary are:',pcnt,' total  words',len(tail))\n",
    "print('total negative words in sentimentDictionary are:',ncnt,' total  words',len(head))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
